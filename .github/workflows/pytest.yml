name: Backend Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - '.github/workflows/pytest.yml'
      - '.github/workflows/ci.yml'  # Also run when main CI changes
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - '.github/workflows/pytest.yml'
      - '.github/workflows/ci.yml'  # Also run when main CI changes
  workflow_dispatch:  # Allow manual triggering

env:
  PYTHON_VERSION: '3.11'

jobs:
  test:
    name: Run Tests & Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: changeme
          MYSQL_DATABASE: miraiworks_test
          MYSQL_USER: changeme
          MYSQL_PASSWORD: changeme
        ports:
          - 3307:3306
        options: >-
          --health-cmd="mysqladmin ping -h localhost -u changeme -pchangeme"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: 'backend/requirements.txt'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y mysql-client-core-8.0

    - name: Install Python dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install faker pytest-xdist pytest-benchmark asyncmy  # Additional test dependencies

    - name: Create test configuration
      working-directory: ./backend
      run: |
        cat > .env.test << EOF
        # Test Environment Configuration
        ENVIRONMENT=test
        SECRET_KEY=test-secret-key-for-pytest-ci-cd-testing
        ALGORITHM=HS256
        ACCESS_TOKEN_EXPIRE_MINUTES=30

        # Database - Use MySQL like local tests
        DATABASE_URL=mysql+asyncmy://changeme:changeme@127.0.0.1:3307/miraiworks_test

        # Redis
        REDIS_URL=redis://localhost:6379/0

        # Email (Mock for tests)
        SMTP_HOST=localhost
        SMTP_PORT=587
        SMTP_USER=test@example.com
        SMTP_PASSWORD=testpassword

        # File Storage (Mock for tests)
        STORAGE_TYPE=local
        STORAGE_PATH=/tmp/test_storage

        # Security
        CORS_ORIGINS=["http://localhost:3000"]
        ALLOWED_HOSTS=["localhost", "127.0.0.1"]

        # Testing flags
        DISABLE_AUTH_FOR_TESTS=false
        MOCK_EXTERNAL_SERVICES=true
        EOF

    - name: Verify Python environment
      working-directory: ./backend
      run: |
        python --version
        pip list | grep -E "(pytest|fastapi|sqlalchemy)"
        python -c "import sys; print('Python path:', sys.path)"

    - name: Run pytest with verbose output
      working-directory: ./backend
      continue-on-error: true
      id: pytest-run
      env:
        PYTHONPATH: .
        ENVIRONMENT: test
      run: |
        export PYTHONPATH=$PWD
        echo "=== Running pytest with comprehensive test suite ==="
        echo "PYTHONPATH: $PYTHONPATH"
        echo "Working directory: $(pwd)"
        echo "Python executable: $(which python)"

        # Run tests with maximum detail for CI debugging
        python -m pytest app/tests/ \
          --verbose \
          --tb=long \
          --strict-markers \
          --strict-config \
          --asyncio-mode=auto \
          --maxfail=5 \
          --durations=10 \
          --cov=app \
          --cov-branch \
          --cov-report=term-missing:skip-covered \
          --cov-report=xml:coverage.xml \
          --cov-report=html:htmlcov \
          --cov-fail-under=40 \
          --junit-xml=pytest-results.xml > pytest-output.log 2>&1
        PYTEST_EXIT=$?
        cat pytest-output.log
        echo "exit_code=$PYTEST_EXIT" >> $GITHUB_OUTPUT
        exit 0

    - name: Test Summary Report
      working-directory: ./backend
      if: always()
      run: |
        echo "=== Test Execution Summary ==="
        if [ -f pytest-results.xml ]; then
          echo "‚úÖ JUnit XML report generated"
        fi
        if [ -f coverage.xml ]; then
          echo "‚úÖ Coverage XML report generated"
        fi
        if [ -d htmlcov ]; then
          echo "‚úÖ HTML coverage report generated"
          echo "Coverage files:"
          ls -la htmlcov/ | head -10
        fi

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pytest-results
        path: |
          backend/pytest-results.xml
          backend/coverage.xml
          backend/pytest-output.log
        retention-days: 30

    - name: Upload coverage HTML report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-html-report
        path: backend/htmlcov/
        retention-days: 30

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        file: ./backend/coverage.xml
        directory: ./backend/
        flags: backend,pytest
        name: miraiworks-backend-coverage
        fail_ci_if_error: false
        verbose: true

    - name: Generate test summary
      if: always()
      run: |
        echo "## üß™ Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Parse test results from pytest output
        if [ -f backend/pytest-output.log ]; then
          echo "### Test Execution Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Extract test counts
          collected=$(grep "collected" backend/pytest-output.log | head -1 | awk '{print $2}')
          passed=$(grep -E "passed|failed|skipped" backend/pytest-output.log | tail -1 | grep -oP '\d+(?= passed)' || echo "0")
          failed=$(grep -E "passed|failed|skipped" backend/pytest-output.log | tail -1 | grep -oP '\d+(?= failed)' || echo "0")
          skipped=$(grep -E "passed|failed|skipped" backend/pytest-output.log | tail -1 | grep -oP '\d+(?= skipped)' || echo "0")

          echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Tests Collected | $collected |" >> $GITHUB_STEP_SUMMARY
          echo "| ‚úÖ Passed | $passed |" >> $GITHUB_STEP_SUMMARY
          echo "| ‚ùå Failed | $failed |" >> $GITHUB_STEP_SUMMARY
          echo "| ‚è≠Ô∏è Skipped | $skipped |" >> $GITHUB_STEP_SUMMARY
          echo "| **Exit Code** | ${{ steps.pytest-run.outputs.exit_code }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.pytest-run.outputs.exit_code }}" != "0" ]; then
            echo "‚ö†Ô∏è **WARNING: Tests failed!** Exit code: ${{ steps.pytest-run.outputs.exit_code }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
        fi

        if [ -f backend/coverage.xml ]; then
          coverage_percent=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('backend/coverage.xml'); root = tree.getroot(); coverage = root.attrib.get('line-rate', '0'); print(f'{float(coverage)*100:.1f}%')" 2>/dev/null || echo "Unknown")
          echo "üìä **Code Coverage**: $coverage_percent" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "üîç **Artifacts Generated**:" >> $GITHUB_STEP_SUMMARY
        echo "- Test results (JUnit XML)" >> $GITHUB_STEP_SUMMARY
        echo "- Coverage report (XML + HTML)" >> $GITHUB_STEP_SUMMARY
        echo "- Detailed logs and traces" >> $GITHUB_STEP_SUMMARY
        echo "- Full pytest output log" >> $GITHUB_STEP_SUMMARY

  # Separate job for test fixtures diagnosis (when tests fail)
  diagnose-fixtures:
    name: Diagnose Test Fixtures
    runs-on: ubuntu-latest
    if: failure()
    needs: test

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      working-directory: ./backend
      run: |
        pip install -r requirements.txt

    - name: Diagnose fixture issues
      working-directory: ./backend
      env:
        PYTHONPATH: .
      run: |
        echo "=== Fixture Diagnosis ==="
        echo "Testing fixture imports and basic functionality..."

        python -c "
        try:
            import sys
            sys.path.append('.')
            print('‚úÖ Python path setup successful')

            from app.tests.conftest import *
            print('‚úÖ Conftest imports successful')

            import pytest
            print('‚úÖ Pytest import successful')

            import pytest_asyncio
            print('‚úÖ Pytest-asyncio import successful')

        except Exception as e:
            print(f'‚ùå Error in fixture setup: {e}')
            import traceback
            traceback.print_exc()
        "

        echo ""
        echo "=== Available test files ==="
        find app/tests/ -name "*.py" -type f | sort

        echo ""
        echo "=== Pytest collection test ==="
        python -m pytest app/tests/ --collect-only -q || echo "Collection failed"